\section{Theory}

\subsection{Linear Regression Theory}
\label{chap:Linear Regression Theory}

\quad \, Consider that we have a data-set that shows the values of an observation $\textbf{y} = [y_0, y_1, .., y_{n-1}]$, the dependent response from a set of explanatory values $\textbf{x} = [x_0, x_1, .., x_{n-1}]$. Since we do not know how to explain $\textbf{y}$ in terms of $\textbf{x}$, in other words, we do not have any known function $\textbf{f}(*)$ underlying this case, but there is somehow a correlation between $\textbf{y}$ and $\textbf{x}$, and no multicollinearity among the values of $x_i$, then a linear regression can be formed.\\

From these assumptions, it is usual to understand that $\textbf{y}$ and $\textbf{x}$ have a linear relationship between each other, and we can create a parametrization of the unknown function $\textbf{f}(*)$ from, for instance, a polynomial of degree ($p$), such that:\\

\begin{equation}
\label{eq1}
y = y(x) \rightarrow y(x_i)=y_i=\tilde{y}_i+\epsilon_i=\sum_{j=0}^{p}\beta_j x_i^j + \epsilon_i
\end{equation}\\

For that, we needed to introduce a coefficient $\beta_i$ and an independently and normally distributed noise $\epsilon_i$, which are the foundation of the linear regression model, hence these raises the set of the following equations:\\

$$y_0&=\beta_0+\beta_1x_0^1+\beta_2x_0^2+\dots+\beta_{n-1}x_0^{p}+\epsilon_0$$
$$y_1&=\beta_0+\beta_1x_1^1+\beta_2x_1^2+\dots+\beta_{n-1}x_1^{p}+\epsilon_1$$
$$y_2&=\beta_0+\beta_1x_2^1+\beta_2x_2^2+\dots+\beta_{n-1}x_2^{p}+\epsilon_2$$
$$&\vdots $$
$$y_{n-1}&=\beta_0+\beta_1x_{n-1}^1+\beta_2x_{n-1}^2+\dots+\beta_{n-1}x_{p}^{n-1}+\epsilon_{n-1}$$\\

We now consider $y_i$ as a stochastic values which approximates to the real $y$. Besides, the interpretation of $y_i$ is the expectation of a response $y_i$ given a explanatory observation $x_i$, such that:  $y_i=E[y_i|x_i]$.\\

Then, we re-writing everything as vectors and matrices:\\

$$\textbf{y} = [y_0,y_1, y_2,..., y_{n-1}]^T$$
$$\boldsymbol{\beta} = [\beta_0,\beta_1, \beta_2,\dots, \beta_{p-1}]^T$$
$$\boldsymbol{\epsilon} = [\epsilon_0,\epsilon_1, \epsilon_2,\dots, \epsilon_{n-1}]^T$$

$$\boldsymbol{X}=
\begin{bmatrix} 
1& x_{0}^1 &x_{0}^2& \dots &x_{0}^{p} \\
1& x_{1}^1 &x_{1}^2& \dots &x_{1}^{p} \\
1& x_{2}^1 &x_{2}^2& \dots &x_{2}^{p} \\                      
\vdots& \vdots &\vdots& \cdots &\vdots \\
1& x_{n-1}^1 &x_{n-1}^2& \dots &x_{n-1}^{p} \\
\end{bmatrix}$$ \\

\noindent where $\textbf{X} \in \mathbb{R}^{n \times p}$ is a design matrix from $p$ quantity of explanatory variables (columns) with $n$ number of samples (rows). \\

Finally, equation (\ref{eq1}) can be written as:\\

\begin{equation}
\label{eq2}
\boldsymbol{y} = \boldsymbol{\tilde{y}} + \boldsymbol{\epsilon} = \boldsymbol{X\beta} + \boldsymbol{\epsilon}
\end{equation}\\

\subsection{Ordinary least squares}
\label{chap:Ordinary least squares}

\qquad \, Ordinary least of squares is a technique of estimating the unknown coefficient $\beta$ from the model.\\

We pick a set of $\beta=(\beta_0, \beta_1, ..., \beta_p)^T$ from the minimization of the residual sum of squares: $RSS(\beta)=\sum^N_{i=1}(y_i-f(x_i))^2=\sum^N_{i=1}(y_i-\beta_0-\sum^p_{j=1}x_{ij}\beta_j)^2$, where the training observations $(x_i, y_i)$ are a representation of independent random draws from their population, or $y_i$'s are conditionally independent given inputs $x_i$. \\

Denote $\textbf{X}$ as the $N\times(p+1)$ matrix where each row is an input vector, and $\textbf{y}$ is the (N-vector, ) of outputs. Then, $RSS(\beta)=(\textbf{y}-\textbf{X}\beta)^T(\textbf{y}-\textbf{X}\beta)$ is a quadratic function with $(p+1)$ coefficients. Assuming that $\textbf{X}$ has full column rank, $\textbf{X}^T\textbf{X}$ is positive definite and differentiating with respect to $\beta$, we obtain:\\

\begin{equation}
\label{eq3}
\hat{\beta}=E[\beta]=(\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T\textbf{y}
\end{equation}\\

\noindent where $\hat{\beta}$ is the optimal beta, or minimized beta.\\

Since we got the minimized $\hat{\beta}=(\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T\textbf{y}$, then the predicted values are\\

\begin{equation}
\label{eq4}
\tilde{\textbf{y}}=E[y]=\textbf{X}\hat{\beta}=\textbf{X}(\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T\textbf{y}
\end{equation}\\

\noindent where it is explained as the projection of $\textbf{y}$ onto the column space of $\textbf{X}$.\\

\subsection{Confidence Interval of the coefficients $\beta$}
\label{chap:Confidence Interval of the coefficients $\beta$}

\qquad \, Assume that the observations $y_i$ are uncorrelated and have constant variance $\sigma^2$, and that the $x_i$ are fixed. Then, the variance-covariance matrix of the least square coefficient estimates is given by: \\

\begin{equation}
\label{eq5}
Var[\hat{\beta}]=(\textbf{X}^T\textbf{X})^{-1}\sigma^2
\end{equation}\\

Where the variance $\sigma^2$ can be estimate by $\hat{\sigma}^2=\frac{1}{N-p-1}\sum^N_{i=1}(y_i-\hat{y}_i)^2$. The $(N-p-1)$ makes the $\hat{\sigma}^2$ an unbiased estimate of $\sigma^2$ by $E[\hat{\sigma}^2]=\sigma^2$.\\

Then, the confidence interval for $\beta$ is\\

\begin{equation}
\label{eq6}
(\hat{\beta}_j-z_{1-\alpha}(\textbf{X}^T\textbf{X})^{-1}\sigma^2, \hat{\beta}_j+ z_{1-\alpha}(\textbf{X}^T\textbf{X})^{-1}\sigma^2)
\end{equation}\\

\subsection{Ridge Regression}
\label{chap:Ridge Regression}

\quad \, The OLS technique has a low bias but a large variance. It also treats all parameters with the same weight, not considering that some predictors may be more important than others. Then, Ridge and Lasso take place.\\

To improve the accuracy of a model, we can reduce or increase some of the coefficients. Ridge regression is a technique that shrinks the coefficients, some of then turning to zero, by attributing a penalty to their size.\\

We introduce the tuning parameter $\lambda>=0$ that is the amount of shrinkage. Then, the cost function will be:\\

$$C(\boldsymbol{\beta}) = (\boldsymbol{y} - \boldsymbol{X\beta})^T(\boldsymbol{y} - \boldsymbol{X\beta}) + \lambda \boldsymbol{\beta}^T\boldsymbol{\beta}$$\\

\noindent where its penalty is $||\boldsymbol{\beta}||_2^2 = \boldsymbol{\beta}^T\boldsymbol{\beta}$.\\

From it's derivative with respect to $\beta$, we end up with:\\

\begin{equation}
\label{eq7}
\boldsymbol{\hat\beta} = ( \boldsymbol{X}^T\boldsymbol{X} + \lambda \boldsymbol{I} )^{-1} \boldsymbol{X}^T \boldsymbol{y}
\end{equation}\\

\noindent and the outputs $\boldsymbol{\tilde{y}}$ for a given  $\boldsymbol{X}$ is:\\

\begin{equation}
\label{eq8}
\boldsymbol{\tilde{y}} = \boldsymbol{X\beta} = \boldsymbol{X}(\boldsymbol{X}^T\boldsymbol{X}  + \lambda \boldsymbol{I})^{-1}\boldsymbol{X}^T
\end{equation}\\

Note that $\lambda=0$ will results to the OLS.\\

\subsection{Lasso Regression}
\label{chap:Lasso Regression}

\quad \, Another regression technique is called Lasso Regression, or Least Absolute Shrinkage and Selection Operator. Its cost function is:\\

$$C(\boldsymbol{\beta}) = (\boldsymbol{y} - \boldsymbol{X}\beta)^T(\boldsymbol{y} - \boldsymbol{X}\beta) + \lambda \sqrt{\boldsymbol{\beta}^T\boldsymbol{\beta}}$$\\

\noindent where its penalty is $||\boldsymbol{\beta}||_1 = \sqrt{\boldsymbol{\beta}^T\boldsymbol{\beta}}$.\\

It turns out that this penalty transforms the equation into a non-linear form, and then there is no similar linear algebraic solution for $\beta$ as OLS and Ridge. Instead, we use gradient descent to minimize the cost function. \\
