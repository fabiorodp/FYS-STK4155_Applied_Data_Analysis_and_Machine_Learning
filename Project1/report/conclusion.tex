\section{Conclusion}
\label{chap:Conclusion}

\quad \, After an exhaustive study of the various regression models and machine learning techniques, we are prepared to decide which method is most appropriate for each type of circumstance.\\ 

Starting with Franke's data-set, our regression models could predict the surface's height with outstanding precision. In our first test [\ref{chap:Part A of the study}] with the OLS model, we could obtain coefficients of determination at a rate of between 65 and 77 percent for both training and testing predictions. The MSE's score was also pretty good as lower as 0.019 for training and 0.035 for testing sets, but indicating under-fitting in which adjusts would be necessary. For that, part B [\ref{chap:Part B of the study}] presented a GridSearch where we could visualize the testing MSE score's behavior when the complexities increases [\ref{fig:plotB1and2}]. For didactic propose, a bench-mark parameter of 20 samples was defined and then applied different polynomial transformation degrees from 2 to 9. We could visualize an improvement of the testing MSE from 0.035 to 0.025 at the polynomial degree 5 [\ref{fig:plotB1and2}] with that bench-marked parameter. The line-plot in [\ref{fig:plotB5}] showed a relevant detachment between training and testing MSE rates when fitting the model with polynomial degree 6 or higher, meaning a plausible over-fitting. To check that in more detail, a 100x bootstrapping re-sampling technique combined with bias-variance trade-off decomposition was plotted [\ref{fig:plotB6}] and confirmed a rapid growth of the variance of the model after polynomial degree 6. Later, the 5-folds cross-validation [\ref{fig:plotC1-2}] successfully attenuated that spike of the variance of the model for polynomials above 5-6 degrees, and it did decrease the MSE score from 0.025 to 0.021, our best score here at the polynomial degree 5.\\

Now, Ridge regression and its regularization parameter lambda took place in the study of Franke's function. The aim was to beat the previous testing MSE (0.021) by implementing different lambda values. After the GridSearch between several lambdas and polynomial degrees, we picked lambda=0.0001 as our bench-mark parameter, which obtained a 0.024 testing MSE metric at the polynomial degree 7 [\ref{fig:heatmapD2andD3}]. The line-plot of training and testing MSE [\ref{fig:MSEplotD1}] showed that, differently from the OLS model, the model's over-fitting appeared to come later. The bias-variance trade-off with 100x bootstrapping [\ref{fig:BiasplotD2}] confirmed that suspicion and registered that the variance did come later and with less power than in the OLS model [\ref{fig:plotB6}]. This effect proved that the regularization of the Ridge regression had influenced the results. We could provide more complexity (higher polynomial degree) to the model without suffering much over-fitting, but the best testing MSE obtained was higher/worse than what was obtained for the OLS model. Finally, the 5-fold cross-validation procedure [\ref{fig:plotD3andh6}] could get 0.022, at polynomial degree 5, but not better than 0.0219 obtained before.\\

For Lasso regression under Franke's function, we have done the same procedure as Ridge. First, GridSearch to find the best lambda regularization value, which ended up been 0.0001 [\ref{fig:heatmapE2andE3}]. Line-plot with training and testing MSE [\ref{fig:plotE1}] revealed that the metrics were going to its minimum even for higher polynomial degrees, inferring that the over-fitting had not come. The bias-variance trade-off with 100x bootstrapping [\ref{fig:BiasplotE2}] validated this assumption; there was no sign of an increasing variance until polynomial 11. The same was observed for 6-folds cross-validation [\ref{fig:plotE3andh6}], where the best score was 0.028 for polynomial degree 4, which was not better than 0.0219 obtained before.\\

In its turn, we started to analyze the GeoTIF image containing heights of a region near Stavanger. The aim here was to fit the regression models with the design matrix of x and y between 0 and 1, and after predicting the heights. We noticed that the picture needed to be sliced to a smaller piece for didactic propose and avoid expensive computational calculation, then the bench-mark of 20 samples was selected after a GridSearch [\ref{fig:heatmapF1and2}] when it was obtained the testing MSE of 1.1 for polynomial degree 10 under the OLS model. Later, the training and testing MSE line-plot [\ref{fig:plotF1andF2}] combined with the bias-variance with 100x bootstrapping revealed that a likely over-fitting could occur around polynomial degree 8; therefore, the score obtained previously could have been over-estimated. Next, the k-fold cross-validation [\ref{fig:plotF3andH5}] was realized but got much higher testing MSE, around 2, than before.\\

Moreover, the Ridge regression with the same parameters as before got a score of 1.4 [\ref{fig:heatmapFR1and2}] at polynomial degree 11, for lambda equals 0.0001, our bench-marked parameter. We noticed, by training and testing line-plot [\ref{fig:plotFR1and2}] and bias-variance with bootstrapping [\ref{fig:plotFR1and2}], that Ridge's regularization avoided the variance to appear in contrast to the OLS model when was possible to use higher complexity in terms of polynomial degree. Even though the Ridge method's testing MSE score (1.4) was higher than the OLS (1.1), that higher score might be more reliable because OLS's lower score was confirmed as over-estimated compared to the Ridge's higher score that was not affected by the variance. Lastly, the 6-fold cross-validation [\ref{fig:plotFR3andH5}] did not a better job and got scores around 1.7 for polynomial degree 10.\\

In the end, Lasso regression with lambda equals 0.0001 took place and got much higher testing MSE scores for all experiments, for instance, GridSearch (7.9 for polynomial degree 11) [\ref{fig:heatmapFL1and2}], bias-variance with bootstrapping (8.44) [\ref{fig:plotFL1and2}], k-fold cross-validation (6.1) [\ref{fig:plotFL3andH5}]. Nevertheless, these higher scores could occur because of the Lasso's regularization, which, as happened with Ridge, avoided the variance to rise.\\

After all, one can conclude that the bias-variance trade-off is crucial when performing predictions because they determine if the prediction's metrics are under- or over-estimated or influenced by under- or over-fitting. Therefore, after knowing this trade-off, we can pick the best pre-processing, model, regularization, predictions, and metrics. In our cases, for Franke's function, the best approach was 5-folds cross-validation with polynomial degree 5 for 15 number of samples under the OLS model. For the GeoTIF image, the best procedure was polynomial degree 11 for 20 samples under the Ridge method with 0.0001 regularization. Also, for the GeoTIF image, it could have had a better score if a higher polynomial degree was applied under Ridge or Lasso methods. Additionally, if one wanted to predict a higher area (higher number of samples) for the GeoTIF image, it would have been wise to use a higher polynomial degree combined with Ridge or Lasso models with regularization because it slowed the variance to develop, then, the same for over-fitting. These tests could be an excellent topic for future experiments and improvements.\\
