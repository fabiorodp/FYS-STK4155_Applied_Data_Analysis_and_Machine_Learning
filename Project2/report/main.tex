\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{float}
\usepackage{import}
\usepackage[a4paper, total={6in, 9in}]{geometry}
\graphicspath{ {./pictures/} }

\usepackage{makeidx}
\makeindex

\title{UiO - FYS-STK4155: PROJECT 2: Deep Neural Networks for Regression and Classification cases}
\author{\textbf{FÃ¡bio Rodrigues Pereira} \\ \small fabior@uio.no - github: @fabiorod}
\date{November 18, 2020}

\begin{document}
\maketitle
\begin{abstract}
\noindent This project aims to study many different Gradient Descent (GD), Momentum Gradient Descent (GDM), Stochastic Gradient Descent (SGD), Mini-batch Stochastic Gradient Descent (Mini-SGD), Mini-batch Momentum Stochastic Gradient Descent (Mini-SGDM), Neural Network (NN), and Deep Neural Networks (DNN) techniques applied on a GeoTIF terrain data containing region heights from a place near Stavanger in Norway. We are interested in predicting this region's measurements and finding a model that would overcome the accuracy metrics obtained from traditional methods as Ordinary least squares (OLS) and Ridge regression (RIDGE). From project 1, the best testing Mean Squared Error (MSE) achieved for OLS and RIDGE, respectively, was 1.11 and 1.44. In this project 2, we go through all parameters for the models above, like the number of hidden layers, the number of neurons, activation functions, cost functions, weights initialization methods, Learning Rate ($\eta$), decay, lambda 'l2' regularization, batch size, epochs, training elapsed time, and see that the best testing MSE score achieved for an SGD was 2.99, for a MiniSGDM was 4.18, and for a DNN was 7.46. These scores do not perform equally well as the more traditional regression methods, but some practical benefits are considered, for example, the unnecessary design matrix to feed the DNN models and more efficient/lower training/learning time systems.

Furthermore, this project 2 also aims to study the application of Neural Networks on MNIST and Breast Cancer data-sets. Now, the essence of the problems is not regression anymore, but classification. The former, MNIST, is a classical data-set with multi-classes targets among 0-9, and the latter, Breast Cancer, is a typical data-set with binary target classes between 0-1. Hence, in this part of project 2, we go through all parameters for the models Multi-Layer Perceptron (MLP) and Logistic Regression. For the MNIST data-set, we will see that the best testing accuracy prediction of 97.2 percent was achieved using an MLP model with the Sigmoid activation function and Accuracy-score cost function. For the Breast Cancer data-set, the best testing accuracy prediction of 99 percent was acquired using an MLP model with the Softmax activation function and Accuracy-score cost function.
\end{abstract}

\clearpage
\thispagestyle{empty}

\tableofcontents

\clearpage
\thispagestyle{empty}

\import{./}{introduction.tex}

\clearpage
\thispagestyle{empty}

\import{./}{theory.tex}

\clearpage
\thispagestyle{empty}

\import{./}{discussion.tex}

\clearpage
\thispagestyle{empty}

\import{./}{conclusion.tex}

\clearpage
\thispagestyle{empty}

\import{./}{bibliography.tex}

\end{document}
